{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PURPOSE OF THE NOTEBOOK**\n",
    "\n",
    " This notebook is designed to develop and train a predictive model aimed at optimizing power consumption in a laptop while minimizing the impact on system performance. The primary goal is to create a model that can accurately estimate power savings and suggest optimizations that balance power efficiency with system performance.\n",
    "\n",
    "## **The Step 0. What are we going to predict?**\n",
    "\n",
    " The objective of the model is to predict `estimated_power_savings`. This feature represents the potential power savings that can be achieved through various optimizations. In addition to focusing on power savings, it is crucial to identify features that are critical to maintaining system performance. These features include:\n",
    "\n",
    " - **CPU-related Features:**\n",
    "   - `cpu_power_state`: Represents the power states of the CPU, including active and idle times. This is important as it directly impacts power consumption and performance.\n",
    "   - `frequency_stats`: Indicates the CPU frequency statistics and how often each frequency is used. Higher frequencies generally mean better performance but increased power consumption.\n",
    "   - `average_cpu_frequency`: The average CPU frequency over the monitoring period. This provides insight into the overall CPU performance.\n",
    "   - `cpu_temperature`: Higher temperatures can lead to thermal throttling, reducing performance to prevent overheating.\n",
    "   - `cpu_utilization_avg`: The average CPU utilization percentage, reflecting how much the CPU is being used.\n",
    "\n",
    " - **GPU-related Features:**\n",
    "   - `gpu_power_usage`: Power usage of the GPU, a significant component of overall power consumption.\n",
    "   - `gpu_core_clock`: GPU core clock frequency, indicating the speed of the GPU.\n",
    "   - `gpu_memory_clock`: GPU memory clock frequency, affecting memory performance.\n",
    "   - `gpu_temperature`: Similar to CPU temperature, it impacts thermal management and potential throttling.\n",
    "   - `gpu_utilization`: GPU utilization percentage, reflecting how much the GPU is being used.\n",
    "   - `gpu_memory_utilization`: Indicates how much of the GPU memory is being utilized.\n",
    "\n",
    " - **Display-related Features:**\n",
    "   - `display_backlight_power`: Power consumption by the display backlight. The display is a significant power consumer, and managing its power usage can yield considerable savings.\n",
    "\n",
    "These features are critical because they collectively represent the primary components affecting both power consumption and performance. By carefully analyzing these features, we can ensure that our model suggests optimizations that do not significantly degrade system performance.\n",
    "\n",
    "## **The Step 0.5. How are we going to evaluate our selected model?**\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "To evaluate the performance of our predictive models, we will use the following metrics:\n",
    "\n",
    "- **Root Mean Squared Error (RMSE):**\n",
    "  - **Definition:** RMSE is the square root of the average of squared differences between the predicted and actual values. It provides a measure of the average magnitude of the error.\n",
    "\n",
    "  - **Why RMSE?**\n",
    "    - **Interpretability:** RMSE is easy to interpret as it is in the same units as the target variable, `estimated_power_savings`.\n",
    "    - **Error Sensitivity:** RMSE is sensitive to large errors due to the squaring of differences, making it effective for identifying models that have larger deviations from actual values.\n",
    "\n",
    "- **Mean Absolute Error (MAE):**\n",
    "  - **Definition:** MAE is the average of the absolute differences between the predicted and actual values. It provides a straightforward measure of prediction accuracy.\n",
    "\n",
    "  - **Why MAE?**\n",
    "    - **Robustness to Outliers:** MAE is less sensitive to outliers compared to RMSE, as it does not square the error terms.\n",
    "    - **Simplicity:** MAE is simple to calculate and interpret, providing a clear view of the average prediction error.\n",
    "\n",
    "### Justification for Using RMSE and MAE\n",
    "\n",
    "- **Comprehensive Evaluation:** Using both RMSE and MAE allows for a comprehensive evaluation of the models' performance. RMSE will help identify models with larger prediction errors, while MAE will provide an overall measure of prediction accuracy.\n",
    "- **Balance of Sensitivity and Robustness:** The combination of RMSE and MAE balances sensitivity to large errors (RMSE) and robustness to outliers (MAE). This dual approach ensures that our model selection considers both aspects of prediction accuracy.\n",
    "- **Applicability to Regression Tasks:** Both metrics are standard evaluation measures for regression tasks, making them appropriate for our objective of predicting `estimated_power_savings`.\n",
    "\n",
    "By employing these performance metrics, we aim to select the model that best balances accuracy and robustness, ultimately achieving our goal of optimizing power consumption while maintaining system performance.\n",
    "\n",
    "> Do note that, although we are going to evaluate based on these two metrics, we are only going to\n",
    "> use *RMSE* to do the **hyperparameter tuning**. The *MAE* will be a secondary metric.\n",
    "\n",
    "## **The Model Solution. What kind of predictive task do we have? How are we going to optimize for multiple features?**\n",
    "\n",
    " The predictive task at hand is a regression task, where we aim to predict a continuous target variable, `estimated_power_savings`. However, our goal is multi-objective: we want to optimize power consumption while also considering system performance.\n",
    "\n",
    " To address this, we will explore several modeling options:\n",
    "\n",
    "### **Linear Regression**\n",
    " - **Advantages:**\n",
    "   - Simple to implement and interpret.\n",
    "   - Fast training time.\n",
    "   - Works well with linearly separable data.\n",
    " - **Disadvantages:**\n",
    "   - Limited to linear relationships.\n",
    "   - Can underperform with complex datasets.\n",
    "\n",
    "### **Decision Trees**\n",
    " - **Advantages:**\n",
    "   - Easy to interpret and visualize.\n",
    "   - Can handle non-linear relationships.\n",
    "   - Requires little data preprocessing. (we already preprocessed anyways)\n",
    " - **Disadvantages:**\n",
    "   - Prone to overfitting.\n",
    "   - Can be unstable with small variations in data.\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    " - **Advantages:**\n",
    "\n",
    "   - Reduces overfitting by averaging multiple decision trees.\n",
    "   - Can handle large datasets and high-dimensional data.\n",
    "   - Provides feature importance scores.\n",
    "\n",
    " - **Disadvantages:**\n",
    "   - Longer training time.\n",
    "   - Less interpretable compared to individual decision trees.\n",
    "\n",
    "### **Gradient Boosting Machines (GBM)**\n",
    " - **Advantages:**\n",
    "   - Often achieves high predictive performance.\n",
    "   - Can handle various types of data (continuous, categorical).\n",
    "   - Provides feature importance scores.\n",
    " - **Disadvantages:**\n",
    "   - Computationally intensive.\n",
    "   - Requires careful tuning of hyperparameters.\n",
    "\n",
    "### **XGBoost**\n",
    "\n",
    " XGBoost (Extreme Gradient Boosting) is a powerful and popular implementation of gradient boosting algorithms. It is particularly known for its performance and efficiency.\n",
    "\n",
    " - **Advantages:**\n",
    "   - High predictive accuracy.\n",
    "   - Handles missing data and provides regularization to prevent overfitting.\n",
    "   - Efficient memory usage and parallel processing capabilities.\n",
    "   - Provides feature importance scores, which can help in feature selection.\n",
    " - **Disadvantages:**\n",
    "   - Requires careful hyperparameter tuning.\n",
    "   - Can be complex to interpret compared to simpler models.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Achieving Multi-Objective Optimization\n",
    "\n",
    "To achieve the goal of optimizing for multiple objectives—power savings and maintaining system performance—we can explore several strategies:\n",
    "\n",
    "### Weighted Loss Function\n",
    "\n",
    "  - **Approach:**\n",
    "\n",
    "    - Combine multiple objectives into a single loss function by assigning weights to each objective.\n",
    "    - For example, the loss function could be a weighted sum of power savings error and performance degradation.\n",
    "  - **Advantages:**\n",
    "    - Simple to implement.\n",
    "    - Provides flexibility in adjusting the trade-off between objectives.\n",
    "  - **Disadvantages:**\n",
    "    - Requires careful tuning of weights.\n",
    "    - May not capture complex relationships between objectives.\n",
    "\n",
    "### Multi-Output Regression\n",
    "\n",
    "  - **Approach:**\n",
    "    - Train a model to predict multiple targets simultaneously, such as `estimated_power_savings` and performance metrics (e.g., CPU utilization, GPU utilization).\n",
    "    - Use algorithms that support multi-output regression, like Random Forest Regressor or XGBoost.\n",
    "  - **Advantages:**\n",
    "    - Can capture relationships between multiple targets.\n",
    "    - Efficiently handles multiple objectives in a single model.\n",
    "  - **Disadvantages:**\n",
    "    - Increased model complexity.\n",
    "    - Requires more computational resources.\n",
    "\n",
    "### Pareto Optimization\n",
    "\n",
    "  - **Approach:**\n",
    "    - Use Pareto optimization to identify solutions that offer the best trade-offs between multiple objectives.\n",
    "    - Generate a Pareto front, representing the set of non-dominated solutions where no objective can be improved without degrading another.\n",
    "  - **Advantages:**\n",
    "    - Provides a clear view of trade-offs between objectives.\n",
    "    - Helps in selecting optimal solutions based on preferences.\n",
    "  - **Disadvantages:**\n",
    "    - Computationally intensive.\n",
    "    - Requires more complex optimization algorithms.\n",
    "\n",
    "### Reinforcement Learning (RL)\n",
    "  - **Approach:**\n",
    "    - Implement an RL agent that learns to optimize power consumption and performance through interactions with the environment.\n",
    "    - Define a reward function that balances power savings and performance.\n",
    "  - **Advantages:**\n",
    "    - Adaptive and can handle dynamic environments.\n",
    "    - Learns optimal strategies over time.\n",
    "  - **Disadvantages:**\n",
    "    - Complex to implement.\n",
    "    - Requires significant computational resources and time for training.\n",
    "\n",
    "In this notebook, we will start by focusing on weighted loss function and multi-output regression approaches. These methods provide a good balance of simplicity and effectiveness, making them suitable for our initial implementation. Depending on the results, we may explore more advanced techniques like Pareto optimization and reinforcement learning in future iterations.\n",
    "\n",
    "In summary, this notebook will guide us through the process of developing a predictive model to optimize power consumption while maintaining system performance. We will explore different modeling approaches, evaluate their advantages and disadvantages, and ultimately focus on implementing XGBoost due to its powerful capabilities in handling complex datasets and achieving high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --- \n",
    " \n",
    " ## Step-by-Step Process\n",
    "\n",
    " ### 1. Model Training\n",
    "\n",
    " In this step, we will train various regression models to predict `estimated_power_savings`. The process includes:\n",
    "\n",
    " - **Splitting the Data:** Divide the dataset into training and testing sets to evaluate the model's performance. A common split is 80% training and 20% testing.\n",
    " - **Training Models:** Train multiple regression models, including Linear Regression, Decision Trees, Random Forest, Gradient Boosting Machines (GBM), and XGBoost. Each model will be trained using the training set.\n",
    " - **Evaluating Models:** Assess the performance of each model using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). This will help us understand how well the models predict `estimated_power_savings`.\n",
    "\n",
    "### 2. Hyperparameter Tuning\n",
    "\n",
    " To enhance the performance of our selected models, we will perform hyperparameter tuning:\n",
    "\n",
    " - **Grid Search:** Systematically search through a predefined set of hyperparameters to find the combination that yields the best performance. This method involves training the model with different hyperparameter combinations and evaluating their performance.\n",
    " - **Random Search:** Randomly sample hyperparameters from a defined range and evaluate model performance. This approach can be more efficient than grid search, especially when dealing with a large number of hyperparameters.\n",
    "\n",
    " ### 3. Multi-Objective Optimization\n",
    "\n",
    " Given our dual objectives—optimizing power savings while maintaining system performance—we will implement strategies to handle multiple objectives:\n",
    "\n",
    " - **Weighted Loss Function:** Combine power savings and performance into a single loss function by assigning weights to each objective. This allows us to balance the trade-off between power savings and performance degradation. The loss function can be fine-tuned to prioritize one objective over the other based on the desired outcome.\n",
    " - **Multi-Output Regression:** Train models that can predict multiple targets simultaneously, such as `estimated_power_savings` and performance metrics (e.g., CPU utilization, GPU utilization). This approach captures the relationship between power savings and performance, enabling more informed optimizations.\n",
    "\n",
    " ### 4. Model Evaluation and Validation\n",
    "\n",
    " After training the models, we will validate their performance to ensure robustness and generalizability:\n",
    "\n",
    " - **Cross-Validation:** Use cross-validation techniques (e.g., k-fold cross-validation) to evaluate model performance on different subsets of the data. This helps in assessing the model's ability to generalize to unseen data.\n",
    " - **Performance Comparison:** Compare the performance of different models based on validation metrics. This involves analyzing the trade-offs between power savings and performance to select the best model.\n",
    "\n",
    "\n",
    "\n",
    " ### 5. Final Model Implementation\n",
    "\n",
    " With the optimal hyperparameters identified, we will implement the final model:\n",
    "\n",
    " - **Training with Optimal Hyperparameters:** Retrain the model on the entire training dataset using the best hyperparameters identified during tuning.\n",
    " - **Testing on Hold-Out Set:** Evaluate the final model on the hold-out test set to assess its performance in predicting `estimated_power_savings` while maintaining system performance.\n",
    " - **Analysis and Interpretation:** Analyze the results to understand the impact of different features on power savings and performance. This includes examining feature importance scores and the model's predictions.\n",
    "\n",
    " ### 6. Future Work\n",
    "\n",
    " Based on the results, we may explore additional techniques and improvements:\n",
    "\n",
    " - **Advanced Optimization Techniques:** Investigate methods like Pareto optimization to better handle the trade-offs between power savings and performance. Pareto optimization identifies non-dominated solutions where improving one objective cannot degrade the other.\n",
    " - **Reinforcement Learning (RL):** Implement RL agents to dynamically optimize power consumption and performance based on real-time data. This approach allows for continuous learning and adaptation to changing conditions.\n",
    " - **Continuous Monitoring and Updating:** Continuously monitor the model's performance and update it with new data to maintain accuracy and relevance. This involves setting up a feedback loop to incorporate new insights and improve the model over time.\n",
    "\n",
    " By following this detailed step-by-step process, we aim to develop a robust and efficient model that optimizes power consumption while maintaining system performance. The comprehensive approach ensures that we carefully consider multiple objectives and select the best solution for our goal.\n",
    "\n",
    "<br>\n",
    "\n",
    " --- \n",
    "\n",
    " <br>\n",
    "\n",
    "\n",
    "# **STEP 1 AND 2. MODEL TRAINING AND HYPERPARAMETER TUNING**\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Loading Data and checking that it loaded OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DATASET!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"laptop_stats_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_strength (dBm)</th>\n",
       "      <th>tx_power (dBm)</th>\n",
       "      <th>device_power_usage (W)</th>\n",
       "      <th>gpu_power_usage (W)</th>\n",
       "      <th>display_backlight_power (W)</th>\n",
       "      <th>idle_stats (%)</th>\n",
       "      <th>frequency_stats (GHz)</th>\n",
       "      <th>wakeups_per_sec (Hz)</th>\n",
       "      <th>battery_discharge_rate (W)</th>\n",
       "      <th>energy_consumed (Wh)</th>\n",
       "      <th>...</th>\n",
       "      <th>device_name_Device_10</th>\n",
       "      <th>device_name_Device_11</th>\n",
       "      <th>device_name_Device_2</th>\n",
       "      <th>device_name_Device_3</th>\n",
       "      <th>device_name_Device_4</th>\n",
       "      <th>device_name_Device_5</th>\n",
       "      <th>device_name_Device_6</th>\n",
       "      <th>device_name_Device_7</th>\n",
       "      <th>device_name_Device_8</th>\n",
       "      <th>device_name_Device_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.210941</td>\n",
       "      <td>-0.174946</td>\n",
       "      <td>-0.429449</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>-1.581139</td>\n",
       "      <td>0.115323</td>\n",
       "      <td>-0.141395</td>\n",
       "      <td>0.062007</td>\n",
       "      <td>-0.246961</td>\n",
       "      <td>0.099170</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.149429</td>\n",
       "      <td>-1.302987</td>\n",
       "      <td>-0.184735</td>\n",
       "      <td>1.199539</td>\n",
       "      <td>-1.264911</td>\n",
       "      <td>-0.030870</td>\n",
       "      <td>0.046752</td>\n",
       "      <td>-1.245494</td>\n",
       "      <td>-0.103369</td>\n",
       "      <td>-0.136244</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.052561</td>\n",
       "      <td>-1.274283</td>\n",
       "      <td>1.020206</td>\n",
       "      <td>1.194412</td>\n",
       "      <td>-0.948683</td>\n",
       "      <td>-0.261191</td>\n",
       "      <td>1.121649</td>\n",
       "      <td>-1.112252</td>\n",
       "      <td>-0.089217</td>\n",
       "      <td>-1.298594</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.914397</td>\n",
       "      <td>-1.086263</td>\n",
       "      <td>1.070054</td>\n",
       "      <td>1.199539</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.191936</td>\n",
       "      <td>1.121649</td>\n",
       "      <td>-0.978453</td>\n",
       "      <td>-0.170196</td>\n",
       "      <td>-1.298594</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.500518</td>\n",
       "      <td>-1.102761</td>\n",
       "      <td>1.119902</td>\n",
       "      <td>-0.439969</td>\n",
       "      <td>-0.316228</td>\n",
       "      <td>0.963792</td>\n",
       "      <td>1.093617</td>\n",
       "      <td>-1.222282</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.538030</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_strength (dBm)  tx_power (dBm)  device_power_usage (W)  \\\n",
       "0              -0.210941       -0.174946               -0.429449   \n",
       "1              -0.149429       -1.302987               -0.184735   \n",
       "2               1.052561       -1.274283                1.020206   \n",
       "3               0.914397       -1.086263                1.070054   \n",
       "4              -0.500518       -1.102761                1.119902   \n",
       "\n",
       "   gpu_power_usage (W)  display_backlight_power (W)  idle_stats (%)  \\\n",
       "0             0.026303                    -1.581139        0.115323   \n",
       "1             1.199539                    -1.264911       -0.030870   \n",
       "2             1.194412                    -0.948683       -0.261191   \n",
       "3             1.199539                    -0.632456        1.191936   \n",
       "4            -0.439969                    -0.316228        0.963792   \n",
       "\n",
       "   frequency_stats (GHz)  wakeups_per_sec (Hz)  battery_discharge_rate (W)  \\\n",
       "0              -0.141395              0.062007                   -0.246961   \n",
       "1               0.046752             -1.245494                   -0.103369   \n",
       "2               1.121649             -1.112252                   -0.089217   \n",
       "3               1.121649             -0.978453                   -0.170196   \n",
       "4               1.093617             -1.222282                    0.048270   \n",
       "\n",
       "   energy_consumed (Wh)  ...  device_name_Device_10 device_name_Device_11  \\\n",
       "0              0.099170  ...                  False                 False   \n",
       "1             -0.136244  ...                   True                 False   \n",
       "2             -1.298594  ...                  False                 False   \n",
       "3             -1.298594  ...                  False                 False   \n",
       "4              0.538030  ...                  False                 False   \n",
       "\n",
       "   device_name_Device_2  device_name_Device_3  device_name_Device_4  \\\n",
       "0                 False                 False                 False   \n",
       "1                 False                 False                 False   \n",
       "2                 False                 False                 False   \n",
       "3                 False                 False                 False   \n",
       "4                  True                 False                 False   \n",
       "\n",
       "   device_name_Device_5  device_name_Device_6  device_name_Device_7  \\\n",
       "0                 False                 False                 False   \n",
       "1                 False                 False                 False   \n",
       "2                 False                 False                 False   \n",
       "3                 False                 False                 False   \n",
       "4                 False                 False                 False   \n",
       "\n",
       "   device_name_Device_8  device_name_Device_9  \n",
       "0                 False                  True  \n",
       "1                 False                 False  \n",
       "2                 False                 False  \n",
       "3                 False                 False  \n",
       "4                 False                 False  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87648, 62)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['signal_strength (dBm)', 'tx_power (dBm)', 'device_power_usage (W)',\n",
       "       'gpu_power_usage (W)', 'display_backlight_power (W)', 'idle_stats (%)',\n",
       "       'frequency_stats (GHz)', 'wakeups_per_sec (Hz)',\n",
       "       'battery_discharge_rate (W)', 'energy_consumed (Wh)',\n",
       "       'software_activity_impact (W)', 'package_power_state',\n",
       "       'estimated_power_savings (W)', 'thermal_power_management (W)',\n",
       "       'disk_io_power_usage (W)', 'wireless_radio_power_usage (W)',\n",
       "       'power_usage_trends (W)', 'system_base_power (W)',\n",
       "       'wakeups_from_idle (Hz)', 'average_cpu_frequency (GHz)',\n",
       "       'network_latency (ms)', 'gpu_core_clock (GHz)',\n",
       "       'gpu_memory_clock (GHz)', 'gpu_temperature (°C)', 'gpu_power_limit (W)',\n",
       "       'gpu_utilization (%)', 'gpu_memory_utilization (%)',\n",
       "       'cpu_core_clock_avg (GHz)', 'cpu_temperature (°C)',\n",
       "       'cpu_power_limit (W)', 'cpu_utilization_avg (%)',\n",
       "       'cpu_undervolt_offset (V)', 'gpu_undervolt_offset (V)',\n",
       "       'ram_voltage (V)', 'ram_frequency (MHz)', 'ram_utilization (%)',\n",
       "       'ram_cas_latency_cl', 'cpu_power_state_encoded',\n",
       "       'device_address_00:1A:7D:DA:71:00', 'device_address_00:1A:7D:DA:71:01',\n",
       "       'device_address_00:1A:7D:DA:71:02', 'device_address_00:1A:7D:DA:71:03',\n",
       "       'device_address_00:1A:7D:DA:71:04', 'device_address_00:1A:7D:DA:71:05',\n",
       "       'device_address_00:1A:7D:DA:71:06', 'device_address_00:1A:7D:DA:71:07',\n",
       "       'device_address_00:1A:7D:DA:71:08', 'device_address_00:1A:7D:DA:71:09',\n",
       "       'device_address_00:1A:7D:DA:71:10', 'device_address_00:1A:7D:DA:71:11',\n",
       "       'device_name_Device_0', 'device_name_Device_1', 'device_name_Device_10',\n",
       "       'device_name_Device_11', 'device_name_Device_2', 'device_name_Device_3',\n",
       "       'device_name_Device_4', 'device_name_Device_5', 'device_name_Device_6',\n",
       "       'device_name_Device_7', 'device_name_Device_8', 'device_name_Device_9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOING THE TRAINING WITH HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**62 columns, 87648 rows. Means that it loaded successfully**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear Regression with hyperparameter tuning...\n",
      "Linear Regression - RMSE: 0.9992, MAE: 0.9418\n",
      "\n",
      "\n",
      "Training Decision Tree with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:  40%|████      | 2/5 [00:16<00:24,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Decision Tree: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree - RMSE: 0.9996, MAE: 0.9419\n",
      "\n",
      "\n",
      "Training Random Forest with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"laptop_stats_normalized.csv\")\n",
    "\n",
    "# Define the target variable and features\n",
    "target = 'estimated_power_savings (W)'\n",
    "features = ['signal_strength (dBm)', 'tx_power (dBm)', 'device_power_usage (W)', 'gpu_power_usage (W)',\n",
    "            'display_backlight_power (W)', 'idle_stats (%)', 'frequency_stats (GHz)', 'wakeups_per_sec (Hz)',\n",
    "            'battery_discharge_rate (W)', 'energy_consumed (Wh)', 'software_activity_impact (W)',\n",
    "            'thermal_power_management (W)', 'disk_io_power_usage (W)', 'wireless_radio_power_usage (W)',\n",
    "            'power_usage_trends (W)', 'system_base_power (W)', 'wakeups_from_idle (Hz)', 'average_cpu_frequency (GHz)',\n",
    "            'network_latency (ms)', 'gpu_core_clock (GHz)', 'gpu_memory_clock (GHz)', 'gpu_temperature (°C)',\n",
    "            'gpu_power_limit (W)', 'gpu_utilization (%)', 'gpu_memory_utilization (%)', 'cpu_core_clock_avg (GHz)',\n",
    "            'cpu_temperature (°C)', 'cpu_power_limit (W)', 'cpu_utilization_avg (%)', 'cpu_undervolt_offset (V)',\n",
    "            'gpu_undervolt_offset (V)', 'ram_voltage (V)', 'ram_frequency (MHz)', 'ram_utilization (%)',\n",
    "            'ram_cas_latency_cl', 'cpu_power_state_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[features], dataset[target], test_size=0.3, random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grids = {\n",
    "    \"Linear Regression\": {},\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Directory to store best hyperparameters and models\n",
    "output_dir = \"best_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train the models with hyperparameter tuning and evaluate their performance\n",
    "for model_name, model in tqdm(models.items(), desc=\"Training Models\"):\n",
    "    print(f\"\\nTraining {model_name} with hyperparameter tuning...\")\n",
    "    \n",
    "    # Perform hyperparameter tuning if there are parameters to tune\n",
    "    if param_grids[model_name]:\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "    else:\n",
    "        best_model = model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\\n\")\n",
    "    \n",
    "    # Define the results structure\n",
    "    results = {\n",
    "        \"best_params\": best_params,\n",
    "        \"metrics\": {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # JSON file path\n",
    "    json_file_path = os.path.join(output_dir, f\"best_hyperparameters_{model_name.replace(' ', '_')}.json\")\n",
    "    \n",
    "    # Model file path\n",
    "    model_file_path = os.path.join(output_dir, f\"best_model_{model_name.replace(' ', '_')}.joblib\")\n",
    "    \n",
    "    # Check if the file exists and compare the new results with the existing ones\n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            existing_results = json.load(file)\n",
    "        \n",
    "        # Compare RMSE; lower RMSE indicates better performance\n",
    "        if rmse < existing_results[\"metrics\"][\"RMSE\"]:\n",
    "            with open(json_file_path, 'w') as file:\n",
    "                json.dump(results, file, indent=4)\n",
    "                print(f\"Updated best hyperparameters for {model_name}\")\n",
    "            \n",
    "            # Save the model\n",
    "            joblib.dump(best_model, model_file_path)\n",
    "            print(f\"Updated best model for {model_name}\")\n",
    "    else:\n",
    "        with open(json_file_path, 'w') as file:\n",
    "            json.dump(results, file, indent=4)\n",
    "            print(f\"Saved best hyperparameters for {model_name}\")\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(best_model, model_file_path)\n",
    "        print(f\"Saved best model for {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os-systems-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
