{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DOING MULTIOBJECTIVE OPTIMIZATION**\n",
    "\n",
    "---\n",
    "\n",
    "## PURPOSE OF THE NOTEBOOK\n",
    "\n",
    "This notebook aims to address the dual objectives of optimizing power consumption while maintaining system performance for a laptop. In our project, it's critical to achieve significant power savings without compromising on essential performance metrics such as CPU frequency, GPU clock speeds, and display backlight power. Given the constraints of battery life and performance requirements in portable computing, our goal is to find a balanced solution that maximizes power efficiency while ensuring optimal system performance. The best-performing model from the previous analysis, XGBoost, will be leveraged with its optimal hyperparameters as the base model for our multi-objective optimization efforts.\n",
    "\n",
    "## Breakdown of What Will Be DOne In THiss Notebook\n",
    "\n",
    "### 1. Introduction to Multi-Objective Optimization\n",
    "\n",
    "In the context of our project, multi-objective optimization is essential because we have competing goals: reducing power consumption (to extend battery life) and maintaining system performance (to ensure a smooth user experience). These objectives often conflict, as aggressive power-saving measures can lead to degraded performance. Therefore, a balanced approach is required to find an optimal trade-off.\n",
    "\n",
    "### 2. Weighted Loss Function\n",
    "\n",
    "- **Objective**: Combine power savings and performance into a single loss function by assigning weights to each objective.\n",
    "- **Approach**:\n",
    "  - **Why It's Needed**: A weighted loss function allows us to explicitly balance power savings and performance. By adjusting the weights, we can prioritize one objective over the other based on specific use cases or user preferences. For example, during intensive tasks like gaming, performance might be prioritized, whereas, during idle times, power savings might be more important.\n",
    "  - **Implementation**: \n",
    "    - Define a combined loss function that includes both `estimated_power_savings` and performance metrics such as `average_cpu_frequency`, `gpu_core_clock`, `gpu_memory_clock`, and `display_backlight_power`.\n",
    "    - Assign weights to each metric to reflect their relative importance.\n",
    "    - Fine-tune the loss function to achieve the desired balance between power savings and performance.\n",
    "\n",
    "### 3. Multi-Output Regression\n",
    "\n",
    "- **Objective**: Train models that can predict multiple targets simultaneously, such as `estimated_power_savings` and performance metrics (e.g., `average_cpu_frequency`, `gpu_core_clock`, `gpu_memory_clock`, `display_backlight_power`).\n",
    "- **Approach**:\n",
    "  - **Why It's Needed**: Multi-output regression is crucial for understanding the interdependencies between power savings and performance metrics. By predicting multiple outcomes, we can better capture the trade-offs and interactions between different system components, leading to more informed and balanced optimizations.\n",
    "  - **Implementation**:\n",
    "    - Use the best-performing model (XGBoost) to create a multi-output regression framework.\n",
    "    - Train the model to predict both `estimated_power_savings` and performance metrics.\n",
    "    - Evaluate the model's ability to capture the relationship between power savings and performance, ensuring that improvements in power savings do not come at an unacceptable cost to performance.\n",
    "\n",
    "### 4. Advanced Optimization Techniques\n",
    "\n",
    "- **Objective**: Investigate methods like Pareto optimization to better handle the trade-offs between power savings and performance.\n",
    "- **Approach**:\n",
    "  - **Why It's Needed**: Pareto optimization helps identify solutions that offer the best trade-offs between conflicting objectives. In our project, it ensures that any improvement in power savings does not degrade performance beyond an acceptable threshold. This method provides a set of optimal solutions (Pareto front) from which we can choose based on specific requirements or user preferences.\n",
    "  - **Implementation**:\n",
    "    - Implement Pareto optimization to identify non-dominated solutions where improving one objective cannot degrade the other.\n",
    "    - Analyze the Pareto front to select the optimal trade-offs between power savings and performance, providing a clear visual representation of the trade-offs.\n",
    "\n",
    "### 5. Implementation and Evaluation\n",
    "\n",
    "- **Steps**:\n",
    "  - **Implement Weighted Loss Function**: Develop and train the model using a combined loss function that includes power savings and performance metrics.\n",
    "  - **Implement Multi-Output Regression**: Train the XGBoost model to predict both `estimated_power_savings` and performance metrics simultaneously.\n",
    "  - **Implement Pareto Optimization**: Apply Pareto optimization to identify the set of optimal solutions balancing power savings and performance.\n",
    "  - **Evaluate Performance**: Assess the performance of each approach using metrics such as RMSE and MAE for power savings, and similar metrics for performance metrics.\n",
    "  - **Compare Results**: Compare the results of each approach to determine the most effective strategy for balancing power savings and performance.\n",
    "\n",
    "## Best Model and Hyperparameters from our one objective exploration\n",
    "\n",
    "The best-performing model from our previous analysis was XGBoost with the following hyperparameters:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_estimators\": 500,\n",
    "    \"subsample\": 0.9\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Loading Libraries and the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"laptop_stats_normalized.csv\")\n",
    "\n",
    "# Define the target variables and features\n",
    "targets = ['estimated_power_savings (W)', 'average_cpu_frequency (GHz)', 'gpu_core_clock (GHz)', \n",
    "           'gpu_memory_clock (GHz)', 'display_backlight_power (W)']\n",
    "features = ['signal_strength (dBm)', 'tx_power (dBm)', 'device_power_usage (W)', 'gpu_power_usage (W)',\n",
    "            'idle_stats (%)', 'frequency_stats (GHz)', 'wakeups_per_sec (Hz)',\n",
    "            'battery_discharge_rate (W)', 'energy_consumed (Wh)', 'software_activity_impact (W)',\n",
    "            'thermal_power_management (W)', 'disk_io_power_usage (W)', 'wireless_radio_power_usage (W)',\n",
    "            'power_usage_trends (W)', 'system_base_power (W)', 'wakeups_from_idle (Hz)',\n",
    "            'network_latency (ms)', 'gpu_temperature (°C)', 'gpu_power_limit (W)', 'gpu_utilization (%)', \n",
    "            'gpu_memory_utilization (%)', 'cpu_temperature (°C)', 'cpu_power_limit (W)', 'cpu_utilization_avg (%)', \n",
    "            'cpu_undervolt_offset (V)', 'gpu_undervolt_offset (V)', 'ram_voltage (V)', 'ram_frequency (MHz)', \n",
    "            'ram_utilization (%)', 'ram_cas_latency_cl', 'cpu_power_state_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[features], dataset[targets], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Before we start with the Multi Objective OPtimization\n",
    "\n",
    "The choice of weights for the weighted loss function in multi-objective optimization is a critical decision that can significantly impact the performance and outcomes of the model.\n",
    "\n",
    "Because of that, we will implement a search of optimal weights for our model, as part of our iterative optimization process.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined loss function\n",
    "def weighted_loss(y_true, y_pred, weights):\n",
    "    power_savings_loss = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n",
    "    performance_loss = sum(weights[i] * mean_squared_error(y_true[:, i+1], y_pred[:, i+1]) for i in range(len(weights)))\n",
    "    return power_savings_loss + performance_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function for Optuna\n",
    "\n",
    "The Optuna study was conducted to optimize the hyperparameters of the XGBoost model and the weights for the performance metrics in the multi-objective optimization task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters for XGBoost. These are the same that we got from the hyperparameter optimization\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, step=0.01)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 500, step=50)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0, step=0.1)\n",
    "    \n",
    "    # Suggest weights for the performance metrics\n",
    "    weight_cpu = trial.suggest_float('weight_cpu', 0.1, 0.5, step=0.05)\n",
    "    weight_gpu_core = trial.suggest_float('weight_gpu_core', 0.05, 0.3, step=0.05)\n",
    "    weight_gpu_memory = trial.suggest_float('weight_gpu_memory', 0.05, 0.3, step=0.05)\n",
    "    weight_display = trial.suggest_float('weight_display', 0.05, 0.2, step=0.05)\n",
    "    \n",
    "    weights = [weight_cpu, weight_gpu_core, weight_gpu_memory, weight_display]\n",
    "    \n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "        colsample_bytree=colsample_bytree, \n",
    "        learning_rate=learning_rate, \n",
    "        max_depth=max_depth, \n",
    "        n_estimators=n_estimators, \n",
    "        subsample=subsample, \n",
    "        random_state=42\n",
    "    ))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the combined loss\n",
    "    loss = weighted_loss(np.array(y_test), y_pred, weights)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-16 17:02:15,605] A new study created in memory with name: no-name-ab803169-9c24-4df0-b937-3319b98b1da1\n",
      "[I 2024-05-16 17:02:17,823] Trial 0 finished with value: 1.6330228340699058 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.04, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8, 'weight_cpu': 0.30000000000000004, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.2}. Best is trial 0 with value: 1.6330228340699058.\n",
      "[I 2024-05-16 17:02:20,786] Trial 1 finished with value: 1.7028833910414705 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.09, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.3, 'weight_gpu_memory': 0.3, 'weight_display': 0.05}. Best is trial 0 with value: 1.6330228340699058.\n",
      "[I 2024-05-16 17:02:24,893] Trial 2 finished with value: 1.5435846372129634 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.06999999999999999, 'max_depth': 8, 'n_estimators': 100, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.25, 'weight_gpu_memory': 0.25, 'weight_display': 0.05}. Best is trial 2 with value: 1.5435846372129634.\n",
      "[I 2024-05-16 17:02:27,773] Trial 3 finished with value: 1.6135302770374036 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 350, 'subsample': 0.8, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.3, 'weight_display': 0.1}. Best is trial 2 with value: 1.5435846372129634.\n",
      "[I 2024-05-16 17:02:29,465] Trial 4 finished with value: 1.6359330252110345 and parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.08, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.7, 'weight_cpu': 0.2, 'weight_gpu_core': 0.2, 'weight_gpu_memory': 0.25, 'weight_display': 0.05}. Best is trial 2 with value: 1.5435846372129634.\n",
      "[I 2024-05-16 17:02:32,537] Trial 5 finished with value: 1.5512130549410785 and parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.12, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.5, 'weight_cpu': 0.1, 'weight_gpu_core': 0.2, 'weight_gpu_memory': 0.2, 'weight_display': 0.15000000000000002}. Best is trial 2 with value: 1.5435846372129634.\n",
      "[I 2024-05-16 17:02:41,150] Trial 6 finished with value: 1.5399487945523551 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.02, 'max_depth': 8, 'n_estimators': 400, 'subsample': 0.7, 'weight_cpu': 0.2, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.2, 'weight_display': 0.2}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:09,099] Trial 7 finished with value: 1.7773015146581699 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.04, 'max_depth': 9, 'n_estimators': 450, 'subsample': 0.5, 'weight_cpu': 0.45000000000000007, 'weight_gpu_core': 0.25, 'weight_gpu_memory': 0.25, 'weight_display': 0.05}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:11,338] Trial 8 finished with value: 1.907490390840374 and parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.18000000000000002, 'max_depth': 3, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.45000000000000007, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.25, 'weight_display': 0.15000000000000002}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:19,841] Trial 9 finished with value: 1.7318869219229773 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.12, 'max_depth': 9, 'n_estimators': 150, 'subsample': 0.7, 'weight_cpu': 0.45000000000000007, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.25, 'weight_display': 0.05}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:24,334] Trial 10 finished with value: 1.5431106457054125 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.6, 'weight_cpu': 0.35, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.2}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:27,934] Trial 11 finished with value: 1.550548326194558 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250, 'subsample': 0.6, 'weight_cpu': 0.35, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.2}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:52,879] Trial 12 finished with value: 1.5526005515952068 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 300, 'subsample': 0.6, 'weight_cpu': 0.35, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.2}. Best is trial 6 with value: 1.5399487945523551.\n",
      "[I 2024-05-16 17:03:58,276] Trial 13 finished with value: 1.4175416811942128 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.04, 'max_depth': 7, 'n_estimators': 350, 'subsample': 0.6, 'weight_cpu': 0.2, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.15000000000000002}. Best is trial 13 with value: 1.4175416811942128.\n",
      "[I 2024-05-16 17:04:08,595] Trial 14 finished with value: 1.4708613221334974 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 350, 'subsample': 0.7, 'weight_cpu': 0.2, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.15000000000000002}. Best is trial 13 with value: 1.4175416811942128.\n",
      "[I 2024-05-16 17:04:12,398] Trial 15 finished with value: 1.5232076348363912 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.060000000000000005, 'max_depth': 6, 'n_estimators': 350, 'subsample': 1.0, 'weight_cpu': 0.2, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.15000000000000002}. Best is trial 13 with value: 1.4175416811942128.\n",
      "[I 2024-05-16 17:04:22,939] Trial 16 finished with value: 1.4122550994996348 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.15000000000000002, 'max_depth': 8, 'n_estimators': 350, 'subsample': 0.6, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 16 with value: 1.4122550994996348.\n",
      "[I 2024-05-16 17:04:59,254] Trial 17 finished with value: 1.4425104690741157 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15000000000000002, 'max_depth': 10, 'n_estimators': 400, 'subsample': 0.6, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.2, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 16 with value: 1.4122550994996348.\n",
      "[I 2024-05-16 17:05:02,146] Trial 18 finished with value: 1.688463832974734 and parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.15000000000000002, 'max_depth': 6, 'n_estimators': 250, 'subsample': 0.5, 'weight_cpu': 0.25, 'weight_gpu_core': 0.3, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 16 with value: 1.4122550994996348.\n",
      "[I 2024-05-16 17:05:28,393] Trial 19 finished with value: 1.3248362225565273 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.16, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.6, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 19 with value: 1.3248362225565273.\n",
      "[I 2024-05-16 17:05:56,043] Trial 20 finished with value: 1.5304704309233856 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.15000000000000002, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.25, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 19 with value: 1.3248362225565273.\n",
      "[I 2024-05-16 17:06:04,928] Trial 21 finished with value: 1.3433196276156973 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.17, 'max_depth': 7, 'n_estimators': 450, 'subsample': 0.6, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 19 with value: 1.3248362225565273.\n",
      "[I 2024-05-16 17:06:27,974] Trial 22 finished with value: 1.3820809439471646 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.17, 'max_depth': 8, 'n_estimators': 450, 'subsample': 0.6, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 19 with value: 1.3248362225565273.\n",
      "[I 2024-05-16 17:07:03,509] Trial 23 finished with value: 1.3051921788343046 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.18000000000000002, 'max_depth': 9, 'n_estimators': 450, 'subsample': 0.7, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.1, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:08:02,268] Trial 24 finished with value: 1.3423491718730118 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.8, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:08:56,768] Trial 25 finished with value: 1.3423491718730118 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.8, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:09:33,787] Trial 26 finished with value: 1.4139877432452432 and parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.18000000000000002, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.8, 'weight_cpu': 0.30000000000000004, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:10:30,955] Trial 27 finished with value: 1.3267481483738708 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.13, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.7, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.15000000000000002}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:11:04,041] Trial 28 finished with value: 1.6087473175175657 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.13, 'max_depth': 9, 'n_estimators': 450, 'subsample': 0.7, 'weight_cpu': 0.5, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.1, 'weight_display': 0.15000000000000002}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:12:00,165] Trial 29 finished with value: 1.492622683299337 and parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.13, 'max_depth': 10, 'n_estimators': 400, 'subsample': 0.7, 'weight_cpu': 0.30000000000000004, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.15000000000000002}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:12:36,638] Trial 30 finished with value: 1.3451622507365841 and parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.09999999999999999, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.2, 'weight_display': 0.15000000000000002}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:13:31,044] Trial 31 finished with value: 1.3443479333621184 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.19, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.8, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:14:28,316] Trial 32 finished with value: 1.319753526596038 and parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.17, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.8, 'weight_cpu': 0.25, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.1}. Best is trial 23 with value: 1.3051921788343046.\n",
      "[I 2024-05-16 17:15:17,415] Trial 33 finished with value: 1.1497461533979274 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.16, 'max_depth': 10, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:15:48,995] Trial 34 finished with value: 1.187912143628445 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.16, 'max_depth': 9, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.1, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:16:34,939] Trial 35 finished with value: 1.1875017224340612 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.17, 'max_depth': 10, 'n_estimators': 450, 'subsample': 1.0, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:16:48,009] Trial 36 finished with value: 1.2575915657444148 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.18000000000000002, 'max_depth': 8, 'n_estimators': 400, 'subsample': 1.0, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.1, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:00,047] Trial 37 finished with value: 1.297792111987696 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.16, 'max_depth': 8, 'n_estimators': 400, 'subsample': 1.0, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.15000000000000002, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:02,685] Trial 38 finished with value: 1.2524012693406181 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.14, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:05,741] Trial 39 finished with value: 1.3005131993113896 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.14, 'max_depth': 4, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:08,606] Trial 40 finished with value: 1.2510893012570352 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.11, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:11,661] Trial 41 finished with value: 1.2510893012570352 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.11, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:14,749] Trial 42 finished with value: 1.2517182887398899 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.09, 'max_depth': 4, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:16,858] Trial 43 finished with value: 1.3128024692145335 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.09999999999999999, 'max_depth': 3, 'n_estimators': 400, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:17,867] Trial 44 finished with value: 1.2585499038146168 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.11, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0, 'weight_cpu': 0.1, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:20,899] Trial 45 finished with value: 1.2993337344228242 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.11, 'max_depth': 4, 'n_estimators': 450, 'subsample': 0.9, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.1, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:25,254] Trial 46 finished with value: 1.3706435963455739 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.06999999999999999, 'max_depth': 6, 'n_estimators': 400, 'subsample': 1.0, 'weight_cpu': 0.2, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:26,944] Trial 47 finished with value: 1.4142851250547195 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.08, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.2, 'weight_gpu_memory': 0.1, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:29,853] Trial 48 finished with value: 1.4793508545407366 and parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.12, 'max_depth': 5, 'n_estimators': 350, 'subsample': 1.0, 'weight_cpu': 0.15000000000000002, 'weight_gpu_core': 0.05, 'weight_gpu_memory': 0.3, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n",
      "[I 2024-05-16 17:17:33,231] Trial 49 finished with value: 1.2866693552310993 and parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.16, 'max_depth': 5, 'n_estimators': 350, 'subsample': 0.9, 'weight_cpu': 0.1, 'weight_gpu_core': 0.15000000000000002, 'weight_gpu_memory': 0.05, 'weight_display': 0.05}. Best is trial 33 with value: 1.1497461533979274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'colsample_bytree': 0.5, 'learning_rate': 0.16, 'max_depth': 10, 'n_estimators': 450, 'subsample': 0.9}\n",
      "Best weights: [0.1, 0.05, 0.05, 0.05]\n",
      "Saved Optuna study to best_models/optuna_study.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Save the best parameters\n",
    "best_params = study.best_params\n",
    "best_weights = [best_params['weight_cpu'], best_params['weight_gpu_core'], best_params['weight_gpu_memory'], best_params['weight_display']]\n",
    "best_hyperparams = {\n",
    "    'colsample_bytree': best_params['colsample_bytree'],\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'n_estimators': best_params['n_estimators'],\n",
    "    'subsample': best_params['subsample']\n",
    "}\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "print(f\"Best weights: {best_weights}\")\n",
    "\n",
    "# Save the study results\n",
    "output_dir = \"best_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "study_file_path = os.path.join(output_dir, \"optuna_study.pkl\")\n",
    "joblib.dump(study, study_file_path)\n",
    "print(f\"Saved Optuna study to {study_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the study are as follows:\n",
    "\n",
    "\n",
    "## Best Hyperparameters\n",
    "\n",
    "The best hyperparameters identified for the XGBoost model are:\n",
    "\n",
    "- **colsample_bytree**: 0.5\n",
    "  - This parameter controls the subsample ratio of columns when constructing each tree. A lower value can help in reducing overfitting.\n",
    "\n",
    "- **learning_rate**: 0.16\n",
    "  - The learning rate controls the step size at each iteration while moving toward a minimum of the loss function. A value of 0.1 is commonly used to balance convergence speed and the ability to find a better minimum.\n",
    "\n",
    "- **max_depth**: 10\n",
    "  - This parameter specifies the maximum depth of the trees. A deeper tree can model more complex relationships but may overfit the data. A depth of 9 is a reasonable balance between complexity and generalization.\n",
    "\n",
    "- **n_estimators**: 450\n",
    "  - The number of boosting rounds or trees to be built. More trees can improve performance but also increase training time. 350 trees provide a good balance between performance and training time.\n",
    "\n",
    "- **subsample**: 0.9\n",
    "  - This parameter controls the subsample ratio of the training instances. A value of 0.7 means that 70% of the training data is used to build each tree, helping to prevent overfitting.\n",
    "\n",
    "## Best Weights\n",
    "\n",
    "The best weights for the performance metrics are:\n",
    "\n",
    "- **Weight for average_cpu_frequency**: 0.1\n",
    "  - This indicates the relative importance of maintaining average CPU frequency. A higher weight means more emphasis is placed on this metric during optimization.\n",
    "\n",
    "- **Weight for gpu_core_clock**: 0.05\n",
    "  - This weight reflects the importance of the GPU core clock frequency. A lower weight indicates less emphasis compared to the CPU frequency.\n",
    "\n",
    "- **Weight for gpu_memory_clock**: 0.05\n",
    "  - Similar to the GPU core clock, this weight represents the importance of the GPU memory clock frequency.\n",
    "\n",
    "- **Weight for display_backlight_power**: 0.05\n",
    "  - This weight signifies the importance of the display backlight power consumption. A lower weight suggests less emphasis on this metric compared to CPU frequency.\n",
    "\n",
    "\n",
    "The Optuna study results show a careful balance between different hyperparameters and weights to optimize the model's performance while maintaining system efficiency. The selected hyperparameters and weights are providing us with with a foundation to fine-tune and evaluate in the multi-objective optimization process.\n",
    "\n",
    "These parameters will be used to train the final XGBoost model and evaluate its performance in achieving the dual objectives of optimizing power savings and maintaining system performance.\n",
    "\n",
    "The study results have been saved for future reference and further analysis:\n",
    "- **Study file path**: `best_models/optuna_study.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Training Model with best parameters and final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training XGBoost Model:   0%|          | 0/61353 [00:08<?, ?it/s]\n",
      "100%|██████████| 61353/61353 [4:37:05<00:00,  3.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Weighted Loss: 1.9634\n",
      "estimated_power_savings (W) - RMSE: 1.1758, MAE: 1.0121\n",
      "average_cpu_frequency (GHz) - RMSE: 1.4891, MAE: 1.1007\n",
      "gpu_core_clock (GHz) - RMSE: 1.0523, MAE: 0.9475\n",
      "gpu_memory_clock (GHz) - RMSE: 1.6084, MAE: 1.2601\n",
      "display_backlight_power (W) - RMSE: 1.8678, MAE: 1.5772\n",
      "Saved final multi-output XGBoost model to best_models/multi_output_xgboost_optimized.joblib\n",
      "Saved multi-objective results to best_models/multi_objective_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/felipera/anaconda3/envs/os-systems-final/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/felipera/anaconda3/envs/os-systems-final/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/felipera/anaconda3/envs/os-systems-final/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/felipera/anaconda3/envs/os-systems-final/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/felipera/anaconda3/envs/os-systems-final/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the final model with the best parameters\n",
    "final_model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "    colsample_bytree=best_hyperparams['colsample_bytree'], \n",
    "    learning_rate=best_hyperparams['learning_rate'], \n",
    "    max_depth=best_hyperparams['max_depth'], \n",
    "    n_estimators=best_hyperparams['n_estimators'], \n",
    "    subsample=best_hyperparams['subsample'], \n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "# Add a loading bar to the training process\n",
    "with tqdm(total=len(X_train)) as pbar:\n",
    "    for i in range(len(X_train)):\n",
    "        final_model.fit(X_train[i:i+1], y_train[i:i+1])\n",
    "        pbar.update(1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate the final model using the combined loss function\n",
    "combined_loss = weighted_loss(np.array(y_test), y_pred, best_weights)\n",
    "print(f\"Final Combined Weighted Loss: {combined_loss:.4f}\")\n",
    "\n",
    "# Evaluate the final model for each target\n",
    "results = {}\n",
    "for i, target in enumerate(targets):\n",
    "    rmse = mean_squared_error(y_test[target], y_pred[:, i], squared=False)\n",
    "    mae = mean_absolute_error(y_test[target], y_pred[:, i])\n",
    "    results[target] = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae\n",
    "    }\n",
    "    print(f\"{target} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "# Save the final model and results\n",
    "model_file_path = os.path.join(output_dir, \"multi_output_xgboost_optimized.joblib\")\n",
    "joblib.dump(final_model, model_file_path)\n",
    "print(f\"Saved final multi-output XGBoost model to {model_file_path}\")\n",
    "\n",
    "results_file_path = os.path.join(output_dir, \"multi_objective_results.json\")\n",
    "with open(results_file_path, 'w') as file:\n",
    "    json.dump({\n",
    "        \"best_hyperparams\": best_hyperparams,\n",
    "        \"best_weights\": best_weights,\n",
    "        \"combined_loss\": combined_loss,\n",
    "        \"metrics\": results\n",
    "    }, file, indent=4)\n",
    "print(f\"Saved multi-objective results to {results_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os-systems-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
